"""
Model comparison page for the FEP dashboard.

This module implements the model comparison page showing the performance 
of different models with emphasis on clinical utility.
"""
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import clinical_weights

def show_model_comparison():
    """
    Display the model comparison page.
    """
    st.header("Model Performance Comparison")
    
    st.markdown("""
    This page shows the comparative performance of different machine learning models
    used to predict FEP remission outcomes. The metrics focus on clinical utility,
    with special emphasis on correctly identifying high-risk cases to prevent relapse.
    """)
    
    # Create tabs for different comparison views
    tabs = st.tabs(["Performance Metrics", "ROC Curves", "Clinical Utility", "Threshold Analysis"])
    
    with tabs[0]:
        show_performance_metrics()
    
    with tabs[1]:
        show_roc_curves()
    
    with tabs[2]:
        show_clinical_utility()
    
    with tabs[3]:
        show_threshold_analysis()
    
    # Additional explanation
    st.markdown("""
    ### Interpreting Model Performance
    
    #### Standard vs. Clinical Metrics
    
    Traditional machine learning metrics like accuracy and AUC treat all errors equally. However, in FEP prediction:
    
    - Missing a high-risk case (false negative) can lead to relapse, hospitalization, or self-harm
    - Unnecessarily intensive intervention (false positive) has lower costs like medication side effects
    
    The models presented here have been optimized using **asymmetric cost functions** that penalize missing 
    high-risk cases more heavily than false alarms.
    
    #### Key Findings
    
    1. The **High-Risk Ensemble** model shows the strongest clinical utility, despite slightly lower overall accuracy
    
    2. For **Class 0 (No remission)** and **Class 3 (Early relapse)**, all models have been optimized
       for high sensitivity at the expense of some specificity
       
    3. The **custom thresholds** produce substantial improvements in clinical utility compared to default
       classification thresholds
    """)

def show_performance_metrics():
    """
    Show model performance metrics table.
    """
    st.subheader("Performance Metrics")
    
    # Create sample performance metrics
    metrics_df = pd.DataFrame({
        'Model': ['Logistic Regression', 'Decision Tree', 'Gradient Boosting', 'High-Risk Ensemble'],
        'Accuracy': [0.68, 0.64, 0.72, 0.69],
        'Weighted AUC': [0.76, 0.71, 0.79, 0.81],
        'Clinical Utility': [0.65, 0.60, 0.73, 0.78],
        'Class 0 Sensitivity': [0.65, 0.71, 0.78, 0.84],
        'Class 0 Specificity': [0.82, 0.75, 0.81, 0.76],
        'Class 3 Sensitivity': [0.62, 0.69, 0.75, 0.82],
        'Class 3 Specificity': [0.80, 0.74, 0.79, 0.75],
        'F2 Score': [0.63, 0.59, 0.70, 0.74]
    })
    
    # Highlight the best model in each metric
    def highlight_max(s):
        is_max = s == s.max()
        return ['background-color: #c6ecc6' if v else '' for v in is_max]
    
    st.dataframe(metrics_df.style.apply(highlight_max, subset=metrics_df.columns[1:]))
    
    # Explain metrics
    with st.expander("ðŸ“Š Metric Explanations"):
        st.markdown("""
        - **Accuracy**: Percentage of correctly classified cases
        - **Weighted AUC**: Area Under ROC Curve, weighted by class importance
        - **Clinical Utility**: Custom metric incorporating asymmetric costs of errors
        - **Class 0 Sensitivity**: Ability to correctly identify no-remission cases
        - **Class 0 Specificity**: Ability to correctly exclude no-remission cases
        - **Class 3 Sensitivity**: Ability to correctly identify early-relapse cases
        - **Class 3 Specificity**: Ability to correctly exclude early-relapse cases
        - **F2 Score**: F-beta score with beta=2, giving more weight to recall
        """)
    
    # Compare default vs. optimized thresholds
    st.subheader("Impact of Threshold Optimization")
    
    threshold_comparison = pd.DataFrame({
        'Model': ['Logistic Regression', 'Decision Tree', 'Gradient Boosting', 'High-Risk Ensemble'],
        'Default Clinical Utility': [0.55, 0.52, 0.61, 0.64],
        'Optimized Clinical Utility': [0.65, 0.60, 0.73, 0.78],
        'Improvement': ['+18%', '+15%', '+20%', '+22%']
    })
    
    st.table(threshold_comparison)
    
    st.markdown("""
    **Note:** Threshold optimization substantially improves clinical utility by finding the 
    optimal balance between sensitivity and specificity for each risk class.
    """)

def show_roc_curves():
    """
    Show ROC curves for different models.
    """
    st.subheader("ROC Curves by Model")
    
    # Create tabs for different models
    model_tabs = st.tabs(["High-Risk Ensemble", "Gradient Boosting", "Logistic Regression", "Decision Tree"])
    
    with model_tabs[0]:
        show_model_roc("High-Risk Ensemble", best=True)
    
    with model_tabs[1]:
        show_model_roc("Gradient Boosting")
    
    with model_tabs[2]:
        show_model_roc("Logistic Regression")
    
    with model_tabs[3]:
        show_model_roc("Decision Tree")
    
    st.markdown("""
    **ROC Curve Interpretation:**
    
    - Curves further from the diagonal line indicate better discrimination
    - The High-Risk Ensemble model shows particularly strong performance for high-risk classes (0 and 3)
    - Note that these curves don't reflect the custom thresholds used in deployment, which prioritize sensitivity for high-risk classes
    """)

def show_model_roc(model_name, best=False):
    """
    Show ROC curves for a specific model.
    
    Parameters:
    -----------
    model_name : str
        Name of the model
    best : bool, default=False
        Whether this is the best performing model
    """
    # Create simulated ROC curve data
    fpr_dict = {
        cls: np.linspace(0, 1, 100) for cls in [0, 1, 2, 3, 6]
    }
    
    # Adjust curve shape based on model name and class
    tpr_dict = {}
    auc_dict = {}
    
    # Base performance levels for each model
    if model_name == "High-Risk Ensemble":
        base_aucs = {0: 0.84, 1: 0.79, 2: 0.76, 3: 0.82, 6: 0.74}
    elif model_name == "Gradient Boosting":
        base_aucs = {0: 0.78, 1: 0.81, 2: 0.77, 3: 0.75, 6: 0.73}
    elif model_name == "Logistic Regression":
        base_aucs = {0: 0.74, 1: 0.78, 2: 0.73, 3: 0.72, 6: 0.70}
    else:  # Decision Tree
        base_aucs = {0: 0.71, 1: 0.73, 2: 0.69, 3: 0.71, 6: 0.67}
    
    # Generate TPR values for each class to match the target AUC
    for cls in [0, 1, 2, 3, 6]:
        # Higher AUC = more curved ROC
        target_auc = base_aucs[cls]
        # Generate a curve with the desired AUC
        # This is a simplified way to generate an ROC curve with approximately the desired AUC
        tpr = fpr_dict[cls] ** (1 / (2 * target_auc - 1))
        tpr_dict[cls] = np.clip(tpr, 0, 1)
        # Calculate actual AUC (will be close to target)
        auc_dict[cls] = np.trapz(tpr_dict[cls], fpr_dict[cls])
    
    # Create the plot
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Map classes to better names
    class_names = {
        0: "No Remission",
        1: "Sustained Remission",
        2: "Late Remission",
        3: "Early Relapse",
        6: "Other Pattern"
    }
    
    # Color high-risk classes differently
    colors = {
        0: 'r',  # Red for high risk
        1: 'g',
        2: 'b',
        3: 'r',  # Red for high risk
        6: 'c'
    }
    
    # Plot ROC curve for each class
    for cls in [0, 1, 2, 3, 6]:
        ax.plot(
            fpr_dict[cls], 
            tpr_dict[cls],
            label=f'Class {cls}: {class_names[cls]} (AUC = {auc_dict[cls]:.2f})',
            color=colors[cls],
            linestyle='-' if cls in [0, 3] else '--'  # Solid for high risk, dashed for others
        )
    
    # Add diagonal line
    ax.plot([0, 1], [0, 1], 'k--')
    
    # Customize plot
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title(f'{model_name} ROC Curves by Class' + (' (Best Performer)' if best else ''))
    ax.legend(loc="lower right")
    ax.grid(alpha=0.3)
    
    st.pyplot(fig)
    
    # Show a note about high-risk classes
    if model_name == "High-Risk Ensemble":
        st.success("""
        **Note**: This model is specifically optimized for Classes 0 and 3 (in red), which represent
        the highest risk patterns of non-remission and early relapse. The stronger performance on these
        classes makes this model preferred for clinical use despite slightly lower performance 
        on other classes.
        """)

def show_clinical_utility():
    """
    Show clinical utility analysis.
    """
    st.subheader("Clinical Utility Analysis")
    
    st.markdown("""
    Clinical utility incorporates the asymmetric costs of different types of errors.
    In FEP prediction, missing a high-risk case (false negative) has much higher costs
    than an unnecessary intervention (false positive).
    """)
    
    # Show error cost configuration
    st.subheader("Error Cost Configuration")
    
    # Create dataframe for costs
    costs_df = pd.DataFrame({
        'Error Type': ['False Negative (Missing high-risk case)', 
                      'False Negative (Missing moderate-risk case)',
                      'False Negative (Missing low-risk case)',
                      'False Positive (Unnecessary intervention for high-risk)',
                      'False Positive (Unnecessary intervention for moderate-risk)',
                      'False Positive (Unnecessary intervention for low-risk)'],
        'Relative Cost': [10.0, 5.0, 1.0, 1.0, 2.0, 3.0],
        'Clinical Implication': ['Possible relapse, hospitalization, self-harm',
                                'Possible delayed intervention, partial relapse',
                                'Typically minimal negative outcome',
                                'Medication side effects, increased monitoring',
                                'Moderate overtreatment, potential side effects',
                                'Unnecessary intensive intervention']
    })
    
    st.table(costs_df)
    
    # Show clinical utility by model
    st.subheader("Clinical Utility by Model")
    
    # Create bar chart of clinical utility
    utility_data = {
        'Model': ['Logistic Regression', 'Decision Tree', 'Gradient Boosting', 'High-Risk Ensemble'],
        'Clinical Utility': [0.65, 0.60, 0.73, 0.78]
    }
    
    fig, ax = plt.subplots(figsize=(10, 5))
    bars = ax.bar(utility_data['Model'], utility_data['Clinical Utility'], color='skyblue')
    
    # Highlight the best model
    bars[3].set_color('green')
    
    # Customize plot
    ax.set_ylim([0, 1.0])
    ax.set_ylabel('Clinical Utility Score (higher is better)')
    ax.set_title('Clinical Utility by Model')
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.2f}', ha='center', va='bottom')
    
    st.pyplot(fig)
    
    # Show error analysis by risk level
    st.subheader("Error Analysis by Risk Level")
    
    # Create data for comparison
    models = ['Logistic Regression', 'Decision Tree', 'Gradient Boosting', 'High-Risk Ensemble']
    risk_levels = ['High Risk', 'Moderate Risk', 'Low Risk']
    
    # False negative rates (lower is better)
    fn_rates = {
        'High Risk': [0.35, 0.29, 0.22, 0.16],
        'Moderate Risk': [0.41, 0.38, 0.30, 0.32],
        'Low Risk': [0.28, 0.31, 0.24, 0.29]
    }
    
    # Create multi-bar chart for false negative rates
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = np.arange(len(risk_levels))
    width = 0.2
    multiplier = 0
    
    for model, color in zip(models, ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']):
        offset = width * multiplier
        rects = ax.bar(x + offset, [fn_rates[risk][multiplier] for risk in risk_levels], 
                       width, label=model, color=color)
        multiplier += 1
    
    # Customize plot
    ax.set_ylabel('False Negative Rate (lower is better)')
    ax.set_title('False Negative Rates by Risk Level and Model')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(risk_levels)
    ax.legend(loc='upper right')
    ax.set_ylim([0, 0.5])
    
    # Add grid lines
    ax.grid(axis='y', alpha=0.3)
    
    st.pyplot(fig)
    
    st.markdown("""
    **Key Observations:**
    
    - The **High-Risk Ensemble** achieves the lowest false negative rate for high-risk cases,
      which is the most clinically important error type to minimize
    
    - All models show higher error rates for moderate-risk classes, reflecting the inherent
      difficulty in predicting these intermediate outcomes
    
    - The ensemble approach specifically prioritizes high-risk detection, sometimes at the
      expense of performance on other risk levels
    """)

def show_threshold_analysis():
    """
    Show threshold optimization analysis.
    """
    st.subheader("Threshold Optimization Analysis")
    
    st.markdown("""
    This analysis shows how prediction thresholds were optimized to maximize
    clinical utility, taking into account the asymmetric costs of different error types.
    """)
    
    # Create sample threshold analysis data
    thresholds = np.linspace(0.1, 0.9, 9)
    
    # Class 0 (High Risk - No Remission)
    class0_data = {
        'threshold': thresholds,
        'sensitivity': [0.92, 0.88, 0.84, 0.78, 0.70, 0.61, 0.52, 0.41, 0.30],
        'specificity': [0.45, 0.58, 0.68, 0.76, 0.82, 0.88, 0.92, 0.95, 0.98],
        'total_cost': [158, 129, 112, 110, 121, 139, 168, 202, 241],
        'false_neg_cost': [24, 36, 48, 66, 90, 117, 144, 177, 210],
        'false_pos_cost': [134, 93, 64, 44, 31, 22, 14, 7, 3]
    }
    
    # Find minimum cost threshold for Class 0
    min_cost_idx = np.argmin(class0_data['total_cost'])
    optimal_threshold = class0_data['threshold'][min_cost_idx]
    
    # Create the plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)
    
    # Plot costs
    ax1.plot(class0_data['threshold'], class0_data['total_cost'], 'k-', label='Total Cost')
    ax1.plot(class0_data['threshold'], class0_data['false_neg_cost'], 'r--', label='False Negative Cost')
    ax1.plot(class0_data['threshold'], class0_data['false_pos_cost'], 'b--', label='False Positive Cost')
    
    # Mark optimal threshold
    ax1.axvline(x=optimal_threshold, color='g', linestyle='-', label=f'Optimal Threshold = {optimal_threshold:.1f}')
    
    ax1.set_ylabel('Cost')
    ax1.set_title(f'Threshold Analysis for Class 0 (No Remission)')
    ax1.legend()
    ax1.grid(True)
    
    # Plot metrics
    ax2.plot(class0_data['threshold'], class0_data['sensitivity'], 'r-', label='Sensitivity')
    ax2.plot(class0_data['threshold'], class0_data['specificity'], 'b-', label='Specificity')
    
    # Mark optimal threshold
    ax2.axvline(x=optimal_threshold, color='g', linestyle='-')
    
    ax2.set_xlabel('Threshold')
    ax2.set_ylabel('Metric Value')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    st.pyplot(fig)
    
    st.markdown(f"""
    **Threshold Analysis Interpretation:**
    
    - For Class 0 (No Remission), the optimal threshold is **{optimal_threshold:.1f}** instead
      of the typical 0.5
    
    - This lower threshold increases sensitivity (reduces missed high-risk cases) at the
      cost of lower specificity (more false alarms)
    
    - The minimum cost point balances the high cost of missing high-risk cases (10Ã— penalty)
      against the lower cost of unnecessary interventions
    
    - Similar analyses were performed for each risk class, with higher thresholds
      for low-risk classes where false positives are more costly
    """)
    
    # Show table of optimized thresholds
    optimized_thresholds = pd.DataFrame({
        'Class': ['Class 0: No Remission', 'Class 1: Sustained Remission', 
                  'Class 2: Late Remission', 'Class 3: Early Relapse',
                  'Class 6: Other Pattern'],
        'Risk Level': ['High', 'Low', 'Moderate', 'High', 'Moderate'],
        'Default Threshold': [0.5, 0.5, 0.5, 0.5, 0.5],
        'Optimized Threshold': [0.3, 0.5, 0.4, 0.3, 0.4],
        'Primary Optimization Goal': ['â†‘ Sensitivity', 'â†‘ Specificity', 
                                     'Balance', 'â†‘ Sensitivity', 'Balance']
    })
    
    st.table(optimized_thresholds)
